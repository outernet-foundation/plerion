import json
import sys
from enum import Enum
from importlib.util import module_from_spec, spec_from_file_location
from pathlib import Path
from tempfile import TemporaryDirectory
from textwrap import dedent
from typing import Any, cast

from common.run_command import run_command
from humps import pascalize
from pydantic import create_model
from pydantic.alias_generators import to_snake
from sqlalchemy import inspect as sa_inspect
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy.sql.sqltypes import Enum as SAEnum
from typer import Option, run


def cli(database: str = Option(...), datamodels_path: Path = Option(...)):
    service_dsn = f"postgresql+psycopg://{database}_api_user:password@localhost:55432/{database}"

    _generate_datamodels_for_schema("public", datamodels_path, service_dsn)
    _generate_datamodels_for_schema("auth", datamodels_path, service_dsn)


def _generate_datamodels_for_schema(database_schema: str, models_path: Path, service_dsn: str) -> None:
    generated_table_models_path = models_path / f"{database_schema}_tables.py"
    generated_dto_models_path = models_path / f"{database_schema}_dtos.py"

    print(f"Generating table models for schema: {database_schema}")

    run_command(
        f'uvx --from "sqlacodegen==3.1.1" --with "psycopg[binary]" sqlacodegen '
        f"--schema {database_schema} "
        f"--options use_inflect "
        f"{service_dsn} "
        f'--outfile "{generated_table_models_path}"',
        log=True,
    )

    # Load module generated by sqlacodegen
    spec = spec_from_file_location("tables", generated_table_models_path)
    assert spec is not None and spec.loader is not None
    models_module = module_from_spec(spec)
    spec.loader.exec_module(models_module)

    # Collect all tables models defined in the module
    tables: dict[str, type[DeclarativeBase]] = {}
    for value in vars(models_module).values():
        if hasattr(value, "__table__") and value.__table__ is not None:
            tables[value.__name__] = value
    tables = dict(sorted(tables.items(), key=lambda item: item[0]))

    # Construct DTO json schemas for each tables
    schemas: dict[str, Any] = {}
    inner_schemas: dict[str, Any] = {}

    for table_name, table in tables.items():
        create_fields: dict[str, Any] = {}
        update_fields: dict[str, Any] = {}
        batch_update_fields: dict[str, Any] = {}
        read_fields: dict[str, Any] = {}

        columns = sa_inspect(table).mapper.local_table.columns
        primary_keys = {col.name for col in columns if col.primary_key}

        for column in columns:
            column_type = column.type.python_type

            # Convert SQLAlchemy Enum to Python Enum if needed
            if isinstance(column.type, SAEnum):
                assert column.type.name is not None
                column_type = cast(
                    type[Enum],
                    Enum(pascalize(column.type.name), {pascalize(v): v for v in list(column.type.enums)}, type=str),
                )

            # Skip tenant_id in all DTOs (it is inferred from / implied by the auth context)
            if column.name == "tenant_id":
                continue

            if getattr(column, "nullable", True):
                read_fields[column.name] = (column_type | None, None)
            else:
                read_fields[column.name] = (column_type, ...)

            # Skip read-only fields in create/update/batch update DTOs
            #
            # Ideally we would infer these per table directly from the database schema based on
            # column privileges, but column privileges are not currently used in our schemas
            # because pg-schema-diff does not support them.
            if column.name in ("created_at", "updated_at", "status", "container_id", "container_url"):
                continue

            if column.name in primary_keys:
                batch_update_fields[column.name] = (column_type, ...)
            else:
                update_fields[column.name] = (column_type | None, None)
                batch_update_fields[column.name] = (column_type | None, None)

            if (
                getattr(column, "nullable", True)
                or getattr(column, "default", None) is not None
                or getattr(column, "server_default", None) is not None
            ):
                create_fields[column.name] = (column_type | None, None)
            else:
                create_fields[column.name] = (column_type, ...)

        for schema_name, fields in [
            (f"{table_name}Create", create_fields),
            (f"{table_name}Update", update_fields),
            (f"{table_name}BatchUpdate", batch_update_fields),
            (f"{table_name}Read", read_fields),
        ]:
            schema = create_model(schema_name, **fields).model_json_schema()
            inner_schemas.update(schema.pop("$defs", {}))
            schemas[schema_name] = schema
            schemas[schema_name]["title"] = schema_name

    with TemporaryDirectory() as temporary_directory_name:
        temp_dir_path = Path(temporary_directory_name)
        schema_path = temp_dir_path / "schema.json"
        dtos_path = temp_dir_path / "classes.py"

        schema_path.write_text(
            json.dumps(
                {
                    "$schema": "https://json-schema.org/draft/2020-12/schema",
                    "$id": "urn:combined-dtos",
                    "type": "object",
                    "properties": {},
                    "$defs": {**schemas, **inner_schemas},
                },
                indent=2,
            )
        )

        print(f"Generating DTO classes for schema: {database_schema}")

        run_command(
            f"uv run datamodel-codegen "
            f"--input {schema_path} "
            f"--input-file-type jsonschema "
            f"--output {dtos_path} "
            f"--target-python-version {sys.version_info.major}.{sys.version_info.minor} "
            f"--output-model-type pydantic_v2.BaseModel",
            log=True,
        )

        dtos_text = dtos_path.read_text()
        imports: list[str] = []
        body: list[str] = []
        for line in dtos_text.splitlines():
            if line.strip() == "from __future__ import annotations" or line.strip().startswith("#"):
                continue

            if line.strip().startswith("from") or line.strip().startswith("import"):
                imports.append(line)
                continue

            body.append(line)

        # deduplicate imports
        imports = list(sorted(set(imports)))

        generated_dto_models_path.write_text(
            "from __future__ import annotations"
            + "\n\n"
            + "\n".join([line for line in imports]).strip()
            + "\n\n"
            + "from sqlalchemy import inspect as sa_inspect"
            + "\n\n"
            + f"from .{database_schema}_tables import {', '.join(tables.keys())}\n"
            + "\n\n\n"
            + ("\n".join([line for line in body]).strip() + "\n\n")
            + "\n\n".join([
                dedent(
                    f"""
                    def {to_snake(name)}_from_dto(create: {name}Create) -> {name}:
                        data = create.model_dump(exclude_unset=True, mode="json")
                        return {name}(**data)

                    def {to_snake(name)}_from_dto_overwrite(instance: {name}, create: {name}Create) -> {name}:
                        for field, value in create.model_dump(exclude_unset=True, mode="json").items():
                            setattr(instance, field, value)
                        return instance

                    def {to_snake(name)}_to_dto(instance: {name}) -> {name}Read:
                        column_keys = tuple(attr.key for attr in sa_inspect({name}).mapper.column_attrs)
                        data = {{k: getattr(instance, k) for k in column_keys}}
                        return {name}Read.model_validate(data)

                    def {to_snake(name)}_apply_dto(instance: {name}, update: {name}Update) -> {name}:
                        for field, value in update.model_dump(exclude_unset=True, mode="json").items():
                            setattr(instance, field, value)
                        return instance

                    def {to_snake(name)}_apply_batch_update_dto(instance: {name}, update: {name}BatchUpdate) -> {name}:
                        for field, value in update.model_dump(exclude_unset=True, mode="json").items():
                            setattr(instance, field, value)
                        return instance
                    """
                ).strip()
                for name in tables.keys()
            ])
            + "\n"
        )


def main():
    run(cli)


if __name__ == "__main__":
    main()
