import json
import re
import sys
from ast import ClassDef, parse, unparse
from enum import Enum
from importlib.util import module_from_spec, spec_from_file_location
from pathlib import Path
from tempfile import TemporaryDirectory
from textwrap import dedent
from typing import Any

from common.run_command import run_command
from humps import pascalize
from pydantic import create_model
from pydantic.alias_generators import to_snake
from sqlalchemy import inspect as sa_inspect
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy.sql.sqltypes import Enum as SAEnum
from typer import run

DATABASE = "plerion"
DATAMODELS_PATH = Path(__file__).parents[3] / "packages" / "generated" / "python" / "datamodels"


def cli():
    service_dsn = f"postgresql+psycopg://{DATABASE}_api_user:password@localhost:55432/{DATABASE}"
    _generate_datamodels_for_schema("public", DATAMODELS_PATH / "src" / "datamodels", service_dsn)
    _generate_datamodels_for_schema("auth", DATAMODELS_PATH / "src" / "datamodels", service_dsn)
    run_command(f"uv pip install {DATAMODELS_PATH.resolve().as_posix()}", log=True)


def _generate_datamodels_for_schema(database_schema: str, models_path: Path, service_dsn: str) -> None:
    generated_table_models_path = models_path / f"{database_schema}_tables.py"
    generated_dto_models_path = models_path / f"{database_schema}_dtos.py"
    generated_table_models_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"Generating table models for schema: {database_schema}")

    run_command(
        f'uvx --from "sqlacodegen==3.1.1" --with "psycopg[binary]" sqlacodegen '
        f"--schema {database_schema} "
        f"--options use_inflect "
        f"{service_dsn} "
        f'--outfile "{generated_table_models_path}"',
        log=True,
    )

    _post_process_generated_enums(generated_table_models_path)

    # Format generated table models
    run_command(f"uv run ruff check --fix --select I,F401 {generated_table_models_path}", log=True)
    run_command(f"uv run ruff format {generated_table_models_path}", log=True)

    # Load module generated by sqlacodegen
    spec = spec_from_file_location("tables", generated_table_models_path)
    assert spec is not None and spec.loader is not None
    models_module = module_from_spec(spec)
    spec.loader.exec_module(models_module)

    # Collect all tables models defined in the module
    tables: dict[str, type[DeclarativeBase]] = {}
    for value in vars(models_module).values():
        if hasattr(value, "__table__") and value.__table__ is not None:
            tables[value.__name__] = value
    tables = dict(sorted(tables.items(), key=lambda item: item[0]))

    # Construct DTO json schemas for each tables
    schemas: dict[str, Any] = {}
    inner_schemas: dict[str, Any] = {}

    for table_name, table in tables.items():
        create_fields: dict[str, Any] = {}
        update_fields: dict[str, Any] = {}
        batch_update_fields: dict[str, Any] = {}
        read_fields: dict[str, Any] = {}

        columns = sa_inspect(table).mapper.local_table.columns
        primary_keys = {col.name for col in columns if col.primary_key}

        for column in columns:
            column_type = column.type.python_type

            # Ensure that raw Enums (str) do not appear (should be post-processed into strict Enums)
            if isinstance(column.type, SAEnum) and not issubclass(column_type, Enum):
                raise NotImplementedError(
                    f"Column '{column.name}' is a raw Enum (str) and was not post-processed correctly."
                )

            # Skip tenant_id in all DTOs (it is inferred from / implied by the auth context)
            if column.name == "tenant_id":
                continue

            if getattr(column, "nullable", True):
                read_fields[column.name] = (column_type | None, None)
            else:
                read_fields[column.name] = (column_type, ...)

            # Skip read-only fields in create/update/batch update DTOs
            #
            # Ideally we would infer these per table directly from the database schema based on
            # column privileges, but column privileges are not currently used in our schemas
            # because pg-schema-diff does not support them.
            if column.name in ("created_at", "updated_at", "status", "container_id", "container_url"):
                continue

            if column.name in primary_keys:
                batch_update_fields[column.name] = (column_type, ...)
            else:
                update_fields[column.name] = (column_type | None, None)
                batch_update_fields[column.name] = (column_type | None, None)

            if (
                getattr(column, "nullable", True)
                or getattr(column, "default", None) is not None
                or getattr(column, "server_default", None) is not None
            ):
                create_fields[column.name] = (column_type | None, None)
            else:
                create_fields[column.name] = (column_type, ...)

        for schema_name, fields in [
            (f"{table_name}Create", create_fields),
            (f"{table_name}Update", update_fields),
            (f"{table_name}BatchUpdate", batch_update_fields),
            (f"{table_name}Read", read_fields),
        ]:
            schema = create_model(schema_name, **fields).model_json_schema()
            inner_schemas.update(schema.pop("$defs", {}))
            schemas[schema_name] = schema
            schemas[schema_name]["title"] = schema_name

    with TemporaryDirectory() as temporary_directory_name:
        temp_dir_path = Path(temporary_directory_name)
        schema_path = temp_dir_path / "schema.json"
        dtos_path = temp_dir_path / "classes.py"

        schema_path.write_text(
            json.dumps(
                {
                    "$schema": "https://json-schema.org/draft/2020-12/schema",
                    "$id": "urn:combined-dtos",
                    "type": "object",
                    "properties": {},
                    "$defs": {**schemas, **inner_schemas},
                },
                indent=2,
            )
        )

        print(f"Generating DTO classes for schema: {database_schema}")

        run_command(
            f"uv run --no_workspace "
            f"datamodel-codegen "
            f"--input {schema_path} "
            f"--input-file-type jsonschema "
            f"--output {dtos_path} "
            f"--target-python-version {sys.version_info.major}.{sys.version_info.minor} "
            f"--output-model-type pydantic_v2.BaseModel",
            log=True,
        )

        # datamodel-codegen may generate duplicate Enum definitions, so we need to deduplicate them
        existing_enums = {
            name
            for name, obj in vars(models_module).items()
            if isinstance(obj, type) and issubclass(obj, Enum) and obj is not Enum
        }

        body: list[str] = []
        tree = parse(dtos_path.read_text())
        for node in tree.body:
            # Skip duplicate Enums
            if isinstance(node, ClassDef) and node.name in existing_enums:
                continue

            body.append(unparse(node))

        imports_from_tables = sorted(list(tables.keys()) + list(existing_enums))

        generated_dto_models_path.write_text(
            "from __future__ import annotations"
            + "\n"
            + "from sqlalchemy import inspect as sa_inspect"
            + "\n"
            + f"from .{database_schema}_tables import {', '.join(imports_from_tables)}"
            + "\n"
            + "\n".join(body)
            + "\n"
            + "\n".join([
                dedent(
                    f"""
                    def {to_snake(name)}_from_dto(create: {name}Create) -> {name}:
                        data = create.model_dump(exclude_unset=True, mode="json")
                        return {name}(**data)

                    def {to_snake(name)}_from_dto_overwrite(instance: {name}, create: {name}Create) -> {name}:
                        for field, value in create.model_dump(exclude_unset=True, mode="json").items():
                            setattr(instance, field, value)
                        return instance

                    def {to_snake(name)}_to_dto(instance: {name}) -> {name}Read:
                        column_keys = tuple(attr.key for attr in sa_inspect({name}).mapper.column_attrs)
                        data = {{k: getattr(instance, k) for k in column_keys}}
                        return {name}Read.model_validate(data)

                    def {to_snake(name)}_apply_dto(instance: {name}, update: {name}Update) -> {name}:
                        for field, value in update.model_dump(exclude_unset=True, mode="json").items():
                            setattr(instance, field, value)
                        return instance

                    def {to_snake(name)}_apply_batch_update_dto(instance: {name}, update: {name}BatchUpdate) -> {name}:
                        for field, value in update.model_dump(exclude_unset=True, mode="json").items():
                            setattr(instance, field, value)
                        return instance
                    """
                ).strip()
                for name in tables.keys()
            ])
            + "\n"
        )

        # Format generated DTO models
        run_command(f"uv run ruff check --fix --select I,F401 {generated_dto_models_path}", log=True)
        run_command(f"uv run ruff format {generated_dto_models_path}", log=True)


# vibe code gemini 3
def _post_process_generated_enums(file_path: Path) -> None:
    """
    Parses the generated SQLAlchemy file, converts inline Enum definitions
    into strict Python Enum classes, injects a fully typed helper for values_callable,
    and rewrites the file.
    """
    content = file_path.read_text()

    # 1. Ensure 'import enum' is present
    if "import enum" not in content:
        content = "import enum\n" + content

    # 2. Regex to capture the inline Enum definition
    pattern = re.compile(r"(\w+): Mapped\[str\] = mapped_column\(Enum\((.*?), name=['\"](\w+)['\"]\)(.*?)\)")

    # Explicitly type the list accumulator
    enum_definitions: list[str] = []

    def replacer(match: re.Match[str]) -> str:
        col_name: str = match.group(1)
        values_raw: str = match.group(2)
        enum_db_name: str = match.group(3)
        rest_of_args: str = match.group(4)

        # Clean up values
        enum_values: list[str] = [val.strip().strip("'\"") for val in values_raw.split(",")]

        # Generate Class Name
        class_name: str = str(pascalize(enum_db_name))

        # Build strict Python Enum Class
        class_lines: list[str] = [f"class {class_name}(enum.Enum):"]

        for v in enum_values:
            if not v:
                continue
            key: str = v.upper().replace("-", "_").replace(" ", "_")
            class_lines.append(f"    {key} = '{v}'")

        enum_definitions.append("\n".join(class_lines) + "\n")

        # Return the new column definition with values_callable
        return f"{col_name}: Mapped[{class_name}] = mapped_column(Enum({class_name}, name='{enum_db_name}', values_callable=enum_values){rest_of_args})"

    # 3. Apply the replacement
    new_content = pattern.sub(replacer, content)

    # 4. Insert the new Enum classes AND the typed helper function
    if enum_definitions:
        # CHANGE: Added strict type annotations to the injected helper
        # x: list[enum.Enum] -> inputs are Enums
        # -> list[str]       -> outputs are the string values
        # str(e.value)       -> explicit cast ensures the return type matches
        enum_definitions.insert(
            0, "def enum_values(x: list[enum.Enum]) -> list[str]:\n    return [str(e.value) for e in x]\n"
        )

        lines = new_content.splitlines()
        insert_idx = 0
        for i, line in enumerate(lines):
            if line.startswith("import ") or line.startswith("from "):
                insert_idx = i

        lines.insert(insert_idx + 2, "\n".join(enum_definitions))

        new_content = "\n".join(lines)
        file_path.write_text(new_content)
        print(f"Post-processed Enums in {file_path.name}")


def main():
    run(cli)


if __name__ == "__main__":
    main()
